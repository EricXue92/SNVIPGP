import torch
import gpytorch
from gpytorch.distributions import MultivariateNormal
from gpytorch.kernels import RBFKernel, RQKernel, MaternKernel, ScaleKernel
from gpytorch.means import ConstantMean
from gpytorch.models import ApproximateGP

from gpytorch.variational import (
    CholeskyVariationalDistribution,
    IndependentMultitaskVariationalStrategy,
    VariationalStrategy,
)

from sklearn import cluster

# å‚æ•°:ï¼ˆè®­ç»ƒæ•°æ®, ç”¨äºç‰¹å¾è½¬æ¢çš„ç¥ç»ç½‘ç»œ, inducing pointsçš„æ•°é‡ï¼‰
# å‡½æ•°: è®­ç»ƒæ•°æ® -> ç¥ç»ç½‘ç»œ -> è½¬æ¢åçš„æ•°æ®ç‰¹å¾ +  inducing points çš„æ•°æ®
        # è°ƒç”¨ _get_initial_inducing_points å‡½æ•°ï¼Œå¾—åˆ° inducing points ä½ç½®åæ ‡
        # è°ƒç”¨ _get_initial_lengthscale å‡½æ•°ï¼Œå¾—åˆ°å½¢çŠ¶å‚æ•°
# return: initial inducing points çš„ä½ç½®åæ ‡ å’Œ å½¢çŠ¶å‚æ•°
def initial_values(train_dataset, feature_extractor, n_inducing_points):
    
    steps = 10
    # Generate a random permutation of indices and select the first 1000
    
    # Split the indices into 'steps' number of chunks
    # 1ã€ç”Ÿæˆå’Œè®­ç»ƒæ•°æ®ä¸€æ ·é•¿åº¦çš„æ•°å€¼, æŠŠæ•°ç»„éšæœºæ‰“ä¹±, å–å‰1000ä¸ªå€¼, åˆ†æˆ10ä»½
    # idx æ˜¯åŒ…æ‹¬ æ¯100ä¸ªçš„indexsçš„tuple  
    # Eg. torch.arange(11).chunk(6) -- (tensor([0, 1]), tensor([2, 3]), ...)
    # # https://pytorch.org/docs/stable/generated/torch.chunk.html

    idx = torch.randperm( len(train_dataset) )[:1000].chunk(steps) # return a tuple 
    
    # ç”¨æ¥å­˜å‚¨ æŠŠè®­ç»ƒæ•°æ® ç»è¿‡ç¥ç»ç½‘ç»œæ¢åçš„ è¾“å‡ºæ•°æ®
    f_X_samples = []

    # ä¸€å…±10å—æ•°æ®indexï¼Œ æ¯ä¸€å—æ•°æ® NN è½¬æ¢ï¼Œ
    with torch.no_grad():
        for i in range(steps):
            # torch.stack will introduce a new dimension
            # æŠŠæ¯100ä¸ªæ•° å åŠ åœ¨ä¸€èµ·ï¼Œç»„æˆä¸€ä¸ªX_sample
            X_sample = torch.stack( [ train_dataset[j][0] for j in idx[i] ]) # axis = 0
            if torch.cuda.is_available():
                X_sample = X_sample.cuda()
                feature_extractor = feature_extractor.cuda()
            # X_sample is like a training batch 
            f_X_samples.append( feature_extractor(X_sample).cpu() )  
            
    # torch.cat joins the tensors along an existing dimension without adding any new dimensions.
    # torch.cat() can be seen as an inverse operation for torch.split() and torch.chunk()
    f_X_samples = torch.cat(f_X_samples) # dim=0
    
    # è·å– ç»è¿‡ç‰¹å¾è½¬æ¢å çš„ åˆå§‹inducing points çš„ä½ç½®
    initial_inducing_points = _get_initial_inducing_points(
        f_X_samples.numpy(), n_inducing_points
    )
    # å¾—åˆ° åˆå§‹åŒ–å½¢çŠ¶å‚æ•°å‚æ•° å’Œ inducing points çš„ä½ç½®
    initial_lengthscale = _get_initial_lengthscale(f_X_samples)
    return initial_inducing_points, initial_lengthscale


#### åˆå§‹åŒ–çš„ 10ä¸ª inducing points ä½ç½®ï¼ˆé’ˆå¯¹å·²ç»è½¬æ¢è¿‡çš„ç‰¹å¾ï¼‰
# å‚æ•°: (ç»è¿‡ç¥ç»ç½‘ç»œè½¬æ¢åçš„ç‰¹æ•°æ®ç‰¹å¾, inducing_pointsçš„æ•°é‡)
# å‡½æ•°: æ•°æ®ç‰¹å¾ -> Kmeansæ–¹æ³•ï¼ˆåˆ†æˆ10ä»½ï¼‰--> å¾—åˆ°10ä¸ªcentroidsçš„ä¸­å¿ƒç‚¹ 
# return (num_of_inducing points, ç¥ç»ç½‘ç»œè½¬æ¢åç‰¹å¾ç»´åº¦) E.g. ( 10 , 128 )
def _get_initial_inducing_points(f_X_sample, n_inducing_points):
    # MiniBatchKMeans -> handle large datasets more efficiently by using mini-batches to update the cluster centroids,
    kmeans = cluster.MiniBatchKMeans(
        n_clusters = n_inducing_points, 
        batch_size = n_inducing_points * 10
    )
    kmeans.fit(f_X_sample)
    initial_inducing_points = torch.from_numpy(kmeans.cluster_centers_)
    return initial_inducing_points

# åˆå§‹åŒ–lengthscaleå½¢çŠ¶å‚æ•°ï¼Œlengthscale = æ‰€æœ‰æ•°æ®ç‚¹è·ç¦»ç›¸äº’ä¹‹é—´çš„å¹³å‡å€¼ 
def _get_initial_lengthscale(f_X_samples):
    if torch.cuda.is_available():
        f_X_samples = f_X_samples.cuda()
    # torch.pdist -> Computes the p-norm ï¼ˆ2ï¼‰ distance between every pair of row vectors in the input. 
    #Input shape ï¼ˆ N , M ï¼‰ --> return  (1/2 * N * ï¼ˆN + 1ï¼‰ , ) æ¯2ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»ç»„æˆçš„1ç»´å‘é‡
    initial_lengthscale = torch.pdist(f_X_samples).mean()
    return initial_lengthscale.cpu()

# å®šä¹‰ä¸€ä¸ªè¿‘ä¼¼çš„ é«˜æ–¯è¿‡ç¨‹ æ¨¡å‹
class GP(ApproximateGP):
    def __init__(
        self,
        num_outputs, 
        initial_lengthscale, 
        initial_inducing_points, # (num_of_inducing_points, ç‰¹å¾ç»´åº¦)
        kernel="RBF",
    ):
        n_inducing_points = initial_inducing_points.shape[0]

        if num_outputs > 1:
            
            batch_shape = torch.Size([num_outputs])
            
        else:
            batch_shape = torch.Size([])

        # # Variational/approximate distribution --> å®šä¹‰ä¸€ä¸ªåéªŒåˆ†å¸ƒ
        # define the form of the approximate inducing value posterior q(u)
        
        # This tells us what form the variational distribution q(u) should take
        # handling multiple independent GPs or mini-batch training.
        variational_distribution = CholeskyVariationalDistribution(
            n_inducing_points, batch_shape = batch_shape
        )
        
        # # Variational strategy initialization --> å®šä¹‰ä¸€ä¸ªå¦‚ä½•ä¼˜åŒ–åéªŒq(u)çš„ç­–ç•¥ï¼Œä»è€Œèƒ½å¾—åˆ°q(f)
        # define how to compute ğ‘( ğŸ(ğ—) ) from ğ‘(ğ®)
        
        # This tells us how to transform a distribution q(u) over the inducing point values to 
        # a distribution q(f) over the latent function values for some input x.
        variational_strategy = VariationalStrategy(
            self, initial_inducing_points, variational_distribution
        )
        
        # The IndependentMultitaskVariationalStrategy wraps around an existing VariationalStrategy,
        # extending it to handle multiple tasks.
        
        if num_outputs > 1:
            variational_strategy = IndependentMultitaskVariationalStrategy(
                variational_strategy, num_tasks = num_outputs
            )
            
        super().__init__(variational_strategy)
        
        kwargs = {
            "batch_shape": batch_shape,
        }

        if kernel == "RBF":
            kernel = RBFKernel(**kwargs)
        elif kernel == "Matern12":
            kernel = MaternKernel(nu=1 / 2, **kwargs)
        elif kernel == "Matern32":
            kernel = MaternKernel(nu=3 / 2, **kwargs)
        elif kernel == "Matern52":
            kernel = MaternKernel(nu=5 / 2, **kwargs)
        elif kernel == "RQ":
            kernel = RQKernel(**kwargs)
        else:
            raise ValueError("Specified kernel not known.")

        kernel.lengthscale = initial_lengthscale * torch.ones_like( kernel.lengthscale )
        
        self.mean_module = ConstantMean(batch_shape= batch_shape)
        self.covar_module = ScaleKernel(kernel, batch_shape=batch_shape)
        
        # forward method computes the GP's mean and covariance for input x.
    def forward(self, x):
        mean = self.mean_module(x)
        covar = self.covar_module(x)
        return MultivariateNormal(mean, covar)
    @property
    def inducing_points(self):
        for name, param in self.named_parameters():
            if "inducing_points" in name:
                return param

# åˆå§‹åŒ–å‚æ•°ï¼šfeature_extractor å’Œ gp
# è¿‡ç¨‹ï¼š x -> ç¥ç»ç½‘ç»œ self.feature_extractor(x) -> features -> è¾“å…¥ç»™ å®šä¹‰çš„é«˜æ–¯éšæœºè¿‡ç¨‹æ¨¡å‹ --> å¾…è®­ç»ƒçš„é«˜æ–¯éšæœºæ¨¡å‹
# å¾—åˆ°ï¼š æœ€ç»ˆéœ€è¦è®­ç»ƒçš„é«˜æ–¯éšæœºè¿‡ç¨‹æ¨¡å‹
class DKL(gpytorch.Module):
    def __init__(self, feature_extractor, gp):
        """
        This wrapper class is necessary because ApproximateGP (above) does some magic
        on the forward method which is not compatible with a feature_extractor.
        """
        super().__init__()
        self.feature_extractor = feature_extractor
        self.gp = gp
    def forward(self, x):
        features = self.feature_extractor(x)
        return self.gp(features)